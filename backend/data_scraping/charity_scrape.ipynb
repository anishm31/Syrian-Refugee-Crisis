{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Script for scraping charity data from Wikidata Public API\n",
    "##### Author: Matthew Albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for workflow\n",
    "import requests\n",
    "import copy\n",
    "import json\n",
    "from wikidata_utilities import *\n",
    "from reliefweb_utilities import get_relevant_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of charities to scrape\n",
    "charities = [\"UNHCR\", \"UNICEF\", \"World Food Programme\"]\n",
    "\n",
    "# Define the mapping from label to property ID in Wikidata pages\n",
    "label_to_prop = {\n",
    "  \"org_type\": \"P31\",\n",
    "  \"logo_img\": \"P154\",\n",
    "  \"org_img\": \"P18\",\n",
    "  \"established\": \"P571\",\n",
    "  \"abbreviation\": \"P1813\",\n",
    "  \"parent_org\": \"P749\",\n",
    "  \"headquarters\": \"P159\",\n",
    "  \"awards_received\": \"P166\",\n",
    "  \"website\": \"P856\"\n",
    "}\n",
    "\n",
    "# Define json structure that scraped data will be stored in \n",
    "charity_instance = {\n",
    "  \"name\" : \"\",\n",
    "  \"id\" : \"\",\n",
    "  \"attributes\" : {\n",
    "    \"description\" : \"\",\n",
    "    \"org_type\": [],\n",
    "    \"logo_img\": \"\",\n",
    "    \"org_img\": \"\",\n",
    "    \"established\": \"\",\n",
    "    \"short_name\": \"\",\n",
    "    \"long_name\": \"\",\n",
    "    \"parent_org\": \"\",\n",
    "    \"headquarters\": \"\",\n",
    "    \"hq_country\": \"\",\n",
    "    \"awards_received\": [],\n",
    "    \"website\": \"\",\n",
    "    \"relief_web_id\": \"\",\n",
    "    \"relief_provided\": [],\n",
    "    \"relevant_countries\": []\n",
    "  }\n",
    "}\n",
    "\n",
    "WIKIDATA_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "RELIF_WEB_URL = \"https://api.reliefweb.int/v1/sources\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to scrape wikidata and populate relevant fields\n",
    "def populate_wikidata_fields(json_response, page_id, data):\n",
    "  # Populate fields with data (or empty string if property does not exist for page)\n",
    "  data[\"name\"] = json_response[\"entities\"][page_id][\"labels\"][\"en\"][\"value\"]\n",
    "  data[\"id\"] = page_id\n",
    "  if data[\"attributes\"][\"description\"] == \"\":\n",
    "    # Populate field with description from Wikidata page\n",
    "    data[\"attributes\"][\"description\"] = json_response[\"entities\"][page_id][\"descriptions\"][\"en\"][\"value\"]\n",
    "    \n",
    "  # Use try/except to handle cases where property does not exist for page\n",
    "  \n",
    "  if (data[\"attributes\"][\"org_type\"] == []):\n",
    "    # Try to scrape organization type\n",
    "    try:\n",
    "      org_type_list = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"org_type\"]]\n",
    "      org_types = []\n",
    "      for org in org_type_list:\n",
    "        org_type = get_page_title(org[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "        data[\"attributes\"][\"org_type\"].append(org_type)\n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "  # Try to scrape logo image\n",
    "  try:\n",
    "    img_file = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"logo_img\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "    data[\"attributes\"][\"logo_img\"] = get_img_url(img_file)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # Try to scrape for image related to organization\n",
    "  try:\n",
    "    img_file = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"org_img\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "    data[\"attributes\"][\"org_img\"] = get_img_url(img_file)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # Try to scrape the year the organization was established\n",
    "  try:\n",
    "    date = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"established\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"]\n",
    "    data[\"attributes\"][\"established\"] = date[1:]\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  if data[\"attributes\"][\"short_name\"] == \"\":\n",
    "    # Try to scrape the abbreviation of the organization\n",
    "    try:\n",
    "      data[\"attributes\"][\"abbreviation\"] = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"abbreviation\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"text\"]\n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "  # Try to scrape for the parent organization\n",
    "  try:\n",
    "    parent_org = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"parent_org\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    data[\"attributes\"][\"parent_org\"] = get_page_title(parent_org)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # Try to scrape for headquarters of organization\n",
    "  try:\n",
    "    headquarters = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"headquarters\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    data[\"attributes\"][\"headquarters\"] = get_page_title(headquarters)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # Try to scrape for awards received by organization\n",
    "  try:\n",
    "    awards_list = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"awards_received\"]]\n",
    "    for award in awards_list:\n",
    "      award_dict = {\n",
    "        \"award_name\": \"\",\n",
    "        \"award_date\": \"\"\n",
    "      }\n",
    "      award_name = get_page_title(award[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "      date_property = \"P585\"\n",
    "      award_date = award[\"qualifiers\"][date_property][0][\"datavalue\"][\"value\"][\"time\"][1:]\n",
    "      # Populate fields of award json object\n",
    "      award_dict[\"award_name\"] = award_name\n",
    "      award_dict[\"award_date\"] = award_date\n",
    "      data[\"attributes\"][\"awards_received\"].append(award_dict)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  if data[\"attributes\"][\"website\"] == \"\":\n",
    "    # Try to scrap for website of organization\n",
    "    try:\n",
    "      data[\"attributes\"][\"website\"] = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"website\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "# Function used to scrape data from ReliefWeb API\n",
    "def populate_relief_web_fields(charity, data):\n",
    "  # Define the parameters for the API request\n",
    "  params = {\n",
    "    \"appname\": \"syrianrefugeecrisis\",\n",
    "    \"profile\": \"full\",\n",
    "    \"limit\": 1,\n",
    "    \"query[value]\": charity\n",
    "  }\n",
    "  # Make API request to get organization data\n",
    "  response = requests.get(RELIF_WEB_URL, params=params)\n",
    "  # Very response was a success\n",
    "  if response.status_code == 200 and response.json()[\"count\"] > 0:\n",
    "    # Extract org data object\n",
    "    org_data = response.json()[\"data\"][0]\n",
    "    # Populate fields with relevant data from API response\n",
    "    \n",
    "    # Use try/except to handle cases where field does not exist for organization\n",
    "    try:\n",
    "      data[\"attributes\"][\"relief_web_id\"] = org_data[\"id\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"description\"] = org_data[\"fields\"][\"description\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"org_type\"].append(org_data[\"fields\"][\"type\"][\"name\"])\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"short_name\"] = org_data[\"fields\"][\"shortname\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"long_name\"] = org_data[\"fields\"][\"longname\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"website\"] = org_data[\"fields\"][\"homepage\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"hq_country\"] = org_data[\"fields\"][\"country\"][0][\"name\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"logo_img\"] = org_data[\"fields\"][\"logo\"][\"url\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "  else:\n",
    "    print(f\"ReliefWeb API Request for {charity} failed\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'World Food Programme',\n",
       " 'id': 'Q204344',\n",
       " 'attributes': {'description': 'United Nations branch related to food-assistance',\n",
       "  'org_type': ['International Organization'],\n",
       "  'logo_img': 'https://upload.wikimedia.org/wikipedia/commons/5/59/World_Food_Programme_Logo_Simple.svg',\n",
       "  'org_img': 'https://upload.wikimedia.org/wikipedia/commons/5/5b/World_Food_Programme.jpg',\n",
       "  'established': '1961-12-19T00:00:00Z',\n",
       "  'short_name': 'WFP',\n",
       "  'long_name': 'United Nations World Food Programme',\n",
       "  'parent_org': 'United Nations',\n",
       "  'headquarters': 'Rome',\n",
       "  'hq_country': 'Italy',\n",
       "  'awards_received': [{'award_name': 'Nobel Peace Prize',\n",
       "    'award_date': '2020-00-00T00:00:00Z'}],\n",
       "  'website': 'http://www.wfp.org',\n",
       "  'relief_web_id': '1741',\n",
       "  'relief_provided': [],\n",
       "  'relevant_countries': {'primary_countries': ['ARM',\n",
       "    'LBN',\n",
       "    'SYR',\n",
       "    'IRQ',\n",
       "    'JOR',\n",
       "    'EGY',\n",
       "    'TUR'],\n",
       "   'secondary_countries': ['JPN',\n",
       "    'NLD',\n",
       "    'NOR',\n",
       "    'IRL',\n",
       "    'DEU',\n",
       "    'RUS',\n",
       "    'FRA',\n",
       "    'USA',\n",
       "    'CHN',\n",
       "    'CAN',\n",
       "    'QAT',\n",
       "    'PSE',\n",
       "    'MEX',\n",
       "    'CHE',\n",
       "    'SAU',\n",
       "    'KOR']}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function used to scrape data for a single charity/organization\n",
    "def scrape_charity(charity):\n",
    "  # Create deep copy of charity_instance and use to populate with data\n",
    "  charity_data = copy.deepcopy(charity_instance)\n",
    "  \n",
    "  # First populate data from ReliefWeb\n",
    "  populate_relief_web_fields(charity, charity_data)\n",
    "  \n",
    "  # Populate data from Wikidata\n",
    "  page_id = get_page_id(charity)\n",
    "  page_data = get_page_data(page_id)\n",
    "  populate_wikidata_fields(page_data, page_id, charity_data)\n",
    "  \n",
    "  # Populate relevant countries for charity\n",
    "  charity_data[\"attributes\"][\"relevant_countries\"] = get_relevant_countries(charity, charity_data[\"attributes\"][\"relief_web_id\"])\n",
    "  return charity_data\n",
    "\n",
    "scrape_charity(\"World Food Programme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('charities.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "  print(len(data))\n",
    "\n",
    "# Scrape data for each charity in the master list\n",
    "# charity_instances = []\n",
    "# for charity in charities:\n",
    "#   page_id = get_page_id(charity)\n",
    "#   charity_instances.append(populate_fields(get_page_data(page_id), page_id))\n",
    "\n",
    "# # Write data to json file\n",
    "# json_file_path = \"./models_data/charity_db.json\"\n",
    "# with open(json_file_path, \"w\") as json_file:\n",
    "#   json.dump(charity_instances, json_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
