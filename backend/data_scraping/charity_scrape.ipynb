{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Script for scraping charity data from Wikidata Public API\n",
    "##### Author: Matthew Albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for workflow\n",
    "import requests\n",
    "import copy\n",
    "import time\n",
    "import json\n",
    "from wikidata_utilities import *\n",
    "from reliefweb_utilities import get_relevant_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping from label to property ID in Wikidata pages\n",
    "label_to_prop = {\n",
    "  \"org_type\": \"P31\",\n",
    "  \"logo_img\": \"P154\",\n",
    "  \"org_img\": \"P18\",\n",
    "  \"established\": \"P571\",\n",
    "  \"abbreviation\": \"P1813\",\n",
    "  \"parent_org\": \"P749\",\n",
    "  \"headquarters\": \"P159\",\n",
    "  \"awards_received\": \"P166\",\n",
    "  \"website\": \"P856\"\n",
    "}\n",
    "\n",
    "# Define json structure that scraped data will be stored in \n",
    "charity_instance = {\n",
    "  \"name\" : \"\",\n",
    "  \"id\" : \"\",\n",
    "  \"attributes\" : {\n",
    "    \"description\" : \"\",\n",
    "    \"org_type\": [],\n",
    "    \"logo_img\": \"\",\n",
    "    \"org_img\": \"\",\n",
    "    \"established\": \"\",\n",
    "    \"short_name\": \"\",\n",
    "    \"long_name\": \"\",\n",
    "    \"parent_org\": \"\",\n",
    "    \"headquarters\": \"\",\n",
    "    \"hq_country\": \"\",\n",
    "    \"awards_received\": [],\n",
    "    \"website\": \"\",\n",
    "    \"relief_web_id\": \"\",\n",
    "    \"relief_web_url\": \"\",\n",
    "    \"relief_provided\": [],\n",
    "    \"relevant_countries\": {\n",
    "      \"primary_countries\": [],\n",
    "      \"secondary_countries\": []\n",
    "    },\n",
    "    \"youtube_info\": {\n",
    "      \"channel_id\": \"\",\n",
    "      \"channel_url\": \"\",\n",
    "      \"relevant_videos\": []\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Define outfile path\n",
    "outfile = \"models_data/charity_db.json\"\n",
    "\n",
    "WIKIDATA_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "RELIF_WEB_URL = \"https://api.reliefweb.int/v1/sources\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to scrape wikidata and populate relevant fields\n",
    "def populate_wikidata_fields(json_response, page_id, data):\n",
    "  # Populate fields with data (or empty string if property does not exist for page)\n",
    "  data[\"id\"] = page_id\n",
    "  if data[\"attributes\"][\"description\"] == \"\":\n",
    "    # Populate field with description from Wikidata page\n",
    "    data[\"attributes\"][\"description\"] = json_response[\"entities\"][page_id][\"descriptions\"][\"en\"][\"value\"]\n",
    "    \n",
    "  # Use try/except to handle cases where property does not exist for page\n",
    "  \n",
    "  if (data[\"attributes\"][\"org_type\"] == []):\n",
    "    # Try to scrape organization type\n",
    "    try:\n",
    "      org_type_list = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"org_type\"]]\n",
    "      org_types = []\n",
    "      for org in org_type_list:\n",
    "        org_type = get_page_title(org[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "        data[\"attributes\"][\"org_type\"].append(org_type)\n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "  # Try to scrape logo image\n",
    "  try:\n",
    "    img_file = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"logo_img\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "    data[\"attributes\"][\"logo_img\"] = get_img_url(img_file)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # Try to scrape for image related to organization\n",
    "  try:\n",
    "    img_file = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"org_img\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "    data[\"attributes\"][\"org_img\"] = get_img_url(img_file)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # Try to scrape the year the organization was established\n",
    "  try:\n",
    "    date = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"established\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"]\n",
    "    data[\"attributes\"][\"established\"] = date[1:]\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  if data[\"attributes\"][\"short_name\"] == \"\":\n",
    "    # Try to scrape the abbreviation of the organization\n",
    "    try:\n",
    "      data[\"attributes\"][\"abbreviation\"] = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"abbreviation\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"text\"]\n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "  # Try to scrape for the parent organization\n",
    "  try:\n",
    "    parent_org = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"parent_org\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    data[\"attributes\"][\"parent_org\"] = get_page_title(parent_org)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # Try to scrape for headquarters of organization\n",
    "  try:\n",
    "    headquarters = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"headquarters\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    data[\"attributes\"][\"headquarters\"] = get_page_title(headquarters)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # Try to scrape for awards received by organization\n",
    "  try:\n",
    "    awards_list = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"awards_received\"]]\n",
    "    for award in awards_list:\n",
    "      award_dict = {\n",
    "        \"award_name\": \"\",\n",
    "        \"award_date\": \"\"\n",
    "      }\n",
    "      award_name = get_page_title(award[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "      date_property = \"P585\"\n",
    "      award_date = award[\"qualifiers\"][date_property][0][\"datavalue\"][\"value\"][\"time\"][1:]\n",
    "      # Populate fields of award json object\n",
    "      award_dict[\"award_name\"] = award_name\n",
    "      award_dict[\"award_date\"] = award_date\n",
    "      data[\"attributes\"][\"awards_received\"].append(award_dict)\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  if data[\"attributes\"][\"website\"] == \"\":\n",
    "    # Try to scrap for website of organization\n",
    "    try:\n",
    "      data[\"attributes\"][\"website\"] = json_response[\"entities\"][page_id][\"claims\"][label_to_prop[\"website\"]][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "# Function used to scrape data from ReliefWeb API\n",
    "def populate_relief_web_fields(charity, data, alternate_params=None):\n",
    "  # Define the parameters for the API request\n",
    "  params = {\n",
    "    \"appname\": \"syrianrefugeecrisis\",\n",
    "    \"profile\": \"full\",\n",
    "    \"limit\": 1,\n",
    "    \"query[value]\": charity\n",
    "  }\n",
    "  if alternate_params != None:\n",
    "    params = alternate_params\n",
    "  # Make API request to get organization data\n",
    "  response = requests.get(RELIF_WEB_URL, params=params)\n",
    "  # Very response was a success\n",
    "  if response.status_code == 200 and response.json()[\"count\"] > 0:\n",
    "    # Extract org data object\n",
    "    org_data = response.json()[\"data\"][0]\n",
    "    # Populate fields with relevant data from API response\n",
    "    \n",
    "    # Use try/except to handle cases where field does not exist for organization\n",
    "    try:\n",
    "      data[\"attributes\"][\"relief_web_id\"] = org_data[\"id\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"description\"] = org_data[\"fields\"][\"description\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"org_type\"].append(org_data[\"fields\"][\"type\"][\"name\"])\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"short_name\"] = org_data[\"fields\"][\"shortname\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"long_name\"] = org_data[\"fields\"][\"longname\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"website\"] = org_data[\"fields\"][\"homepage\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"hq_country\"] = org_data[\"fields\"][\"country\"][0][\"name\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"logo_img\"] = org_data[\"fields\"][\"logo\"][\"url\"]\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    try:\n",
    "      data[\"attributes\"][\"relief_web_url\"] = org_data[\"fields\"][\"url_alias\"]\n",
    "    except:\n",
    "      print(f\"Failed to get ReliefWeb URL for {charity}\")\n",
    "      pass\n",
    "    \n",
    "  else:\n",
    "    print(f\"ReliefWeb API Request for {charity} failed\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to scrape data for a single charity/organization\n",
    "def scrape_charity(charity):\n",
    "  # Extract fields from charity dict\n",
    "  charity_name = charity[\"name\"]\n",
    "  relief_list = charity[\"relief\"]\n",
    "  \n",
    "  # Create deep copy of charity_instance and use to populate with data\n",
    "  charity_data = copy.deepcopy(charity_instance)\n",
    "  \n",
    "  # Populate fields that don't need to be scraped\n",
    "  charity_data[\"name\"] = charity_name\n",
    "  charity_data[\"attributes\"][\"relief_provided\"] = relief_list\n",
    "  \n",
    "  # First populate data from ReliefWeb\n",
    "  if charity.get(\"alternate_query\") is not None:\n",
    "    # If alternate query is present, use that to query ReliefWeb API\n",
    "    populate_relief_web_fields(charity[\"alternate_query\"], charity_data)\n",
    "  elif charity.get(\"query_by_id\") is not None:\n",
    "    # Charity requires using ReliefWeb ID to query API for charity data \n",
    "    params = {\n",
    "      \"appname\": \"syrianrefugeecrisis\",\n",
    "      \"profile\": \"full\",\n",
    "      \"limit\": 1,\n",
    "      \"query[fields][]\": \"id\",\n",
    "      \"query[value]\": charity[\"query_by_id\"]\n",
    "    }\n",
    "    populate_relief_web_fields(charity_name, charity_data, params)\n",
    "  else:\n",
    "    populate_relief_web_fields(charity_name, charity_data)\n",
    "  \n",
    "  # Populate data from Wikidata\n",
    "  page_id = get_page_id(charity_name)\n",
    "  page_data = get_page_data(page_id)\n",
    "  populate_wikidata_fields(page_data, page_id, charity_data)\n",
    "  \n",
    "  # Populate relevant countries for charity\n",
    "  charity_data[\"attributes\"][\"relevant_countries\"] = get_relevant_countries(charity_name, charity_data[\"attributes\"][\"relief_web_id\"])\n",
    "  \n",
    "  return charity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape data for all charities\n",
    "def scrape_all_charities():\n",
    "  # Create list to store charity data\n",
    "  charity_data = []\n",
    "  charities = []\n",
    "  # Load list of charities to scrape from json file\n",
    "  with open(\"charities.json\", \"r\") as f:\n",
    "    charities = json.load(f)\n",
    "  # Iterate through charities and scrape data\n",
    "  count = 1\n",
    "  for charity in charities:\n",
    "    charity_name = charity[\"name\"]\n",
    "    print(f\"Scraping data for {charity_name}...{count}/{len(charities)}\")\n",
    "    charity_data.append(scrape_charity(charity))\n",
    "    count += 1\n",
    "    time.sleep(5)\n",
    "  return charity_data\n",
    "\n",
    "# scraped_data = scrape_all_charities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write scraped data to JSON file\n",
    "def write_to_json(data):\n",
    "  # Write scraped data to JSON file\n",
    "  with open(outfile, \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)\n",
    "\n",
    "# write_to_json(scraped_data)\n",
    "# print(len(scraped_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
